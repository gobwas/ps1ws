Pub/Sub && 1M WebSockets
14 Apr 2017
Tags: websocket, mailru, optimizations

Сергей Камардин
MailRu Group
gobwas@gmail.com
https://github.com/gobwas
@gobwas

* 

.image media/image/qr.png
.html media/html/qr.html

* О чем речь?

* Состояние и события

Совсем глобально – речь про состояние и _информацию_о_его_измениниях_ – событиях.

Наличие или отсутствие реакции на события имеет свою цену.
Скорость реакции тоже.

.image media/image/reaction.jpg 340 _

* Publisher/Subscriber

Способ реагирования на изменение состояния при помощи 
шины событий (_event_channel_).

*Издатель* (_publisher_) отправляет множество событий в шину.

*Подписчик* (_subscriber_) интересуется подмножеством событий из шины.

В качестве шины событий может выступать промежуточный сервер (_message_broker_).

.code media/diagram/pubsub.txt

* WebSocket

Бинарный полнодуплексный протокол общения между браузером и сервером.

Стандартизован в 2011г. как [[https://tools.ietf.org/html/rfc6455][RFC6455]].

Использует HTTP Upgrade в качестве рукопожатия с сервером.

Поддерживается всеми современными браузерами.

* Почта

* Состояние

Какие данные содержит _состояние_ почты?

- письма в ящике
- пометки о прочтении
- срок жизни сессии
- ...

* События

Как пользователь узнает об изменении ящика?

Поллинг каждые 2 минуты:

- 3 миллиона запросов в минуту
- 60% ответов - [[https://tools.ietf.org/html/rfc7232#section-4.1][304 Not Modified]]

.image media/image/pollmail.jpeg 330 _

* События

Как хочется, чтобы пользователь узнавал об изменении ящика?

Мгновенные уведомления об изменении состояния ящика:

- 3 миллиона живых соединений;
- 30 тысяч уведомлений в секунду;

.image media/image/wsmail.jpeg 330 _

* План

Как было изначально:
.code media/diagram/history.txt

* План

Pub/Sub в почте:
.code media/diagram/pubsub.txt

* План

Pub/Sub в почте:
.code media/diagram/pubsub_mail.txt

.caption *Storage* - _publisher_, хранилище ящиков пользователей
.caption *Rebus* - _message_broker_, или шина событий
.caption *Notifier* - _subscriber_, WebSocket сервер

* 
.image media/image/simplicity.jpg 500 _
.caption "Thaaat's right, Dude. The beauty of this is its simplicity. If the plan gets too complex something always goes wrong."
.caption _Walter._Big_Lebowski._

* Нюансы

*Плюсы*:

- скорость доставки писем быстрее, чем push в телефонах
- возможность исключить большинство походов на сервер
- множество возможностей по функционалу

*Минусы*:

- есть трудности работы с большим количеством соединений
- есть много трудностей с большим количеством соединений

* Практика

* Pub/Sub роутинг

Фильтрация событий происходит и в rebus (_message_broker_) и в notifier (_subscriber_).

База данных или таблицы и деревья в памяти:

.code media/diagram/trie.txt

* WebSocket

Handshake:

.code media/code/upgrade.txt

Так же поддерживает выбор расширений и выбор протокола уровня прилоежения ([[https://tools.ietf.org/html/rfc6455#section-11.3.2][Sec-WebSocket-Extensions]] и [[https://tools.ietf.org/html/rfc6455#section-11.3.4][Sec-WebSocket-Protocol]])

* WebSocket

Внутри соединения оперирует фреймами:

- ping/pong
- close
- binary
- text

Поддерживает фрагментацию пакетов.

* WebSocket

Все браузеры действительно поддерживают [[https://tools.ietf.org/html/rfc6455][RFC6455]].

Есть особенности:

- Chrome не дожидается ответного `Close` фрейма и сразу закрывает соединение
- Firefox периодически посылает `Ping` фреймы, ожидая в ответ `Pong`
- IE периодически посылает.. `Pong` фреймы (но это не противоречит спецификации)

* Миллион WebSockets

Время жизни соединения – от нескольких секунд до *нескольких*дней*.

При разрыве соединения клиенты переподключаются.

Если что-то идет не так – клиенты начинают self DDOS.

.image media/image/selfddos.png 200 _
.image media/image/cat.jpeg 200 _

* JSONRPC

На уровне приложения используется [[https://jsonrpc.org][JSONRPC]]:
	
	$> { "id": 42, "method": "subscribe", "params": ... }
	$< { "id": 42, "result": ... }

Некоторые технические сообщения:

- `ping` на уровне websocket/jsonrpc
- `shutdown` - при graceful рестартах
- `sleep` - при вынужденных "жестких" рестартах

* Go

* WebSocket

Реализации [[https://tools.ietf.org/html/rfc6455][RFC6455]] в Go:

- [[golang.org/x/net/websocket][x/net/websocket]]
- [[github.com/gorilla/websocket][gorilla/websocket]]
- [[github.com/gobwas/ws][gobwas/ws]] (?!)

.html media/html/plugin.html

* Миллион WebSockets

Реализация соединения в идиомах Go.

.code media/code/memory.go /DECL>/,/DECL</

* Миллион WebSockets

Независимо от библиотеки, реализация в идиомах Go будут потреблять много памяти:

.code media/code/memory.go /IMPL>/,/IMPL</

* Миллион WebSockets

Соединения относительно долго бездействуют.
Какие можно применить оптимизации, зная этот факт?

Что, если бы горутины знали, когда к ним придут данные?	
	
	ReadBuf + WriteBuf = 8192
	2 x Goroutine      = 8192
	ChanBuf            = 4096

Можно было бы освободить около *20GB* памяти (20KB с каждого соединения).

* Как работает go runtime?

* Runtime poll wait

Что происходит, когда мы говорим `conn.Read()`?

	func read(c *conn, p []byte) (int, error) {
		n, err := syscall.Read(c.fd)
		if err == syscall.EAGAIN {
			runtime_pollWait(c.pollDescriptor, 'r')
		}
	}	

- внутри Go сокеты не блокирующие
- на Linux `runtime_poll*` реализованы с помощью `epoll`
- что мешает на использовать epoll для своих нужд? 

* epoll

* epoll

epoll - I/O event notification facility (c) [[http://man7.org/linux/man-pages/man7/epoll.7.html][_man_epoll_]].

Основная идея выглядит так:
	
	ep := epoll.New()

	ep.Add(ch.conn, EPOLLIN | EPOLLONESHOT, func() {
		ch.Receive()
		epoll.Add(ch.conn)
	})

	// Receive reads from net.Conn in blocking mode.
	func (ch *Channel) Receive() error {
		buf := bufio.NewReader(ch.conn) // Allocation only when we need!
		readPacket(buf)
		...
	}

* Контроль за ресурсами

Что, если вдруг все соединения решат отправить нам сообщение?

Тогда реализация с epoll ничем не будет отличаться от ресурсоемкой idiomatic-way реализации.

Решение – пул горутин.

* Goroutine pool

.code media/code/pool.go /START/,/END/

* Goroutine pool

Переиспользование `bufio.Reader` с помощью `sync.Pool` позволяет зафиксировать количество потребляемой памяти.

При желании пул можно сделать "резиновым".

	p := pool.New(128)
	
	epoll.Add(ch.conn, EPOLLIN | EPOLLONESHOT, func() {
		pool.Schedule(ch.Receive)
	})

* Запись 

С `Write()` дело обстоит проще – мы всегда знаем, когда хотим что-то записать в соединение.

Сложный момент – синхронизация отправляемых пакетов:

.code media/code/write.go /START/,/END/

* github.com/gobwas/ws

* Пришло время запилить свою либу

Как уже говорилось выше, в Go существуют две реализации WebSocket'ов.

И обе они [[https://github.com/gorilla/websocket/issues/186][не позволяют]] полностью контролировать работу с `[]byte` и прочими буферами.

Поэтому пришлось реализовать RFC6455 с более низкоуровневым API:

	import "github.com/gobwas/ws"

	http.HandleFunc("/", func(res http.ResponseWriter, req *http.Request) {
		conn, _ := ws.Upgrade(req, res)
		
		ws.ReadFrame(conn) 
		ws.WriteFrame(conn, ws.NewTextFrame(...))
	})
	
* Zero-copy upgrade

Работа со стандартным сервером из `net/http` выглядит примерно так:

	http.ListenAndServe(..., func(w http.ResponseWriter, r *http.Request) {
		conn, rw, err := ws.Upgrade(r, w)
		...
	})

При парсинге запроса и заполнении `http.Request` происходит большое количество копирований.

Если после `Upgrade` не возвращать буфер соединения в `http.putBufioReader`, то это еще 8KB на соединение.

* Zero-copy upgrade

Имея свою библиотеку можно запилить zero-copy upgrade:

	ln, _ := net.Listen("tcp", ":8888")

	for {
		conn, _ := ln.Accept()
		ws.UpgradeConn(conn)
		// ...
	}

Бенчмарки:

	BenchmarkUpgradeHTTP/lowercase-4        300000              5156 ns/op            8576 B/op          9 allocs/op
	BenchmarkUpgradeTCP/lowercase-4        2000000               973 ns/op               0 B/op          0 allocs/op

.caption В случае простого Upgrade без сохранения значений Sec-WebSocket-Protocol и Sec-WebSocket-Extensions.
.caption Код бенчмарков [[https://github.com/gobwas/ws/blob/master/server_test.go#L274][тут]].


* Все вместе

При использовании с `pool` можно вообще не принимать соединения, если ресурсов больше нет:

	ln, _ := net.Listen("tcp", ":8888")

	for {
		err := pool.ScheduleTimeout(func() {
			conn, _ := ln.Accept()
			ws.UpgradeConn(conn)
			// ...
		})
		if err != nil {
			writeNextUpstreamError(conn)	
			time.Sleep(...)
		}
	}

* Итог

* Немного архитектуры

Go приложение запускается в нескольких экземплярах и мониторится супрвизором.

Перед ним стоит nginx, который берет на себя SSL и балансировку между процессами.

Все слои такой организации получились независимыми и масштабируемым:

.code media/code/scale.txt


* Немного статистики

Все описанные выше решения реализованы и работают, хоть пока еще и не всегда, в production.

- живых соединений 2.6 миллиона
- уведомлений от storage 30 тысяч в секунду
- уведомлений по WebSocket 6 тысяч в секунду

* 

.image media/image/wsmail.jpeg 400 _
.caption PROFIT
