Pub/Sub && 1M WebSockets
14 Apr 2017
Tags: websocket, mailru, optimizations

Сергей Камардин
MailRu Group
gobwas@gmail.com
https://github.com/gobwas
@gobwas

\ʕ◔ϖ◔ʔ/

* 

.image media/image/qr.png
.html media/html/02_qr.html

* О чем речь?

* События

Совсем глобально – речь про состояние и *события* – _информацию_об_изменениях_состояния_.

Наличие или отсутствие реакции на события имеет свою цену.
Скорость реакции тоже.

.image media/image/reaction.jpeg 330 _

* Publisher/Subscriber

Способ реагирования на изменение состояния при помощи 
шины событий (_event_channel_).

*Издатель* (_publisher_) отправляет множество событий в шину.

*Подписчик* (_subscriber_) интересуется подмножеством событий из шины.

В качестве шины событий может выступать промежуточный сервер (_message_broker_).

.code media/diagram/01_pubsub.txt

* WebSocket

Бинарный протокол обмена сообщениями между браузером и сервером.
Полнодуплексный – работает в обе стороны.

Стандартизован в 2011г. как [[https://tools.ietf.org/html/rfc6455][RFC6455]].

Использует HTTP Upgrade запрос в качестве рукопожатия с сервером.

Поддерживается всеми современными браузерами.

* Почта

* Состояние

Чем является состояние для любого сервиса почты?

- письма в ящике
- пометки о прочтении
- срок жизни сессии
- ...

* Сегодня

Как пользователь узнает об изменении ящика?

Поллинг каждые 2 минуты:

- 100 тысяч запросов в секунду
- 60% ответов - [[https://tools.ietf.org/html/rfc7232#section-4.1][304 Not Modified]]

.image media/image/pollmail.jpeg 330 _

* Завтра

Мгновенные уведомления об изменении состояния ящика:

- 3 миллиона живых соединений;
- 30 тысяч уведомлений в секунду;

.image media/image/wsmail.jpeg 330 _

* План

Было:
.code media/diagram/00_history.txt

Стало:
.code media/diagram/02_target.txt

* 
.image media/image/01_simplicity.jpg 500 _
.caption "Thaaat's right, Dude. The beauty of this is its simplicity. If the plan gets too complex something always goes wrong."
.caption _Walter._Big_Lebowski._

* Нюансы

*Плюсы*:

- скорость доставки писем быстрее, чем push в телефонах
- возможность исключить большинство походов на сервер
- множество возможностей по функционалу

*Минусы*:

- есть трудности работы с большим количеством соединений
- есть много трудностей с большим количеством соединений

#.code media/diagram/process.txt

* Практика

* Publisher/Subscriber

В связи с большой нагрузкой на сеть стратегии роутинга multicasting и gossiping не рассматривались.

Фильтрацию сообщений можно реализовать прямо в памяти с помощью хэш-таблиц или деревьев.

Или просто взять любую базу данных.

TODO TRIE

* WebSocket

Весьма простой бинарный протокол.

Имеет Ping/Pong. Close/Close со статусами ответов.
Есть фреймы двух видов Binary и Text.
Существует Continuation.

Все браузеры действительно поддерживают [[https://tools.ietf.org/html/rfc6455][RFC6455]].

Но есть особенности:

- Chrome не дожидается ответного `Close` фрейма и сразу закрывает соединение;
- Firefox периодически посылает `Ping` фреймы, ожидая в ответ `Pong`;
- IE периодически посылает.. `Pong` фреймы, что не противоречит спецификации.

Многие предлагают использовать [[https://tools.ietf.org/html/rfc7692][Compression Extensions]].

* WebSocket

Внутри фреймов конверты [[https://jsonrpc.org][JSONRPC]].

TODO messages

* WebSocket

Реализации [[https://tools.ietf.org/html/rfc6455][RFC6455]] в Go:

- [[golang.org/x/net/websocket][x/net/websocket]]
- [[github.com/gorilla/websocket][gorilla/websocket]]
- [[github.com/gobwas/ws][gobwas/ws]]

.html media/html/01_plugin.html

* 1 Миллион Соединений

- Время жизни соединения – от нескольких секунд до нескольких часов;
- Чтобы однозначно понять статус соединения лучше использовать ping на уровне протокола приложения; 
- Если что-то пойдет не так – можно устроить self DDOS;

* 1 Миллион Соединений

Реализации в идиомах Go будут потреблять много памяти:

.code media/code/00_memory.go /START/,/END/

* Как работает runtime?

Что происходит, когда мы говорим `conn.Read()`?

	func read(c *conn, p []byte) (int, error) {
		n, err := syscall.Read(c.fd)
		if err == syscall.EAGAIN {
			runtime_pollWait(c.pollDescriptor, 'r')
		}
	}	

- внутри Go сокеты не блокирующие;
- на Linux `runtime_poll*` реализованы с помощью `epoll`;
- что мешает на использовать epoll для своих нужд? 

* epoll

* epoll

epoll - I/O event notification facility (c) [[http://man7.org/linux/man-pages/man7/epoll.7.html][_man_epoll_]].

Основная идея выглядит так:

	func (ch *Channel) Receive() error {
		buf := bufio.NewReader(ch.conn) // Allocation only when need!
		for {
			readPacket(buf)
			// ...
		}
	}

	ep := epoll.New()

	// Add add always with EPOLLONESHOT.
	ep.Add(ch.conn, EPOLLIN, func() {
		if err := ch.Receive(); err == nil {
			epoll.Resume(ch.conn)		
		}
	})

* Больше контроля за ресурсами

Что если вдруг большинство соединений решат отправить нам сообщение?

Тогда реализация с epoll ничем не будет отличаться от ресурсоемкой idiomatic-way реализации.

Решение – пул горутин.

* Больше контроля за ресурсами

.code media/code/01_pool.go /START/,/END/

* Больше контроля за ресурсами

Переиспользование `bufio.Reader` с помощью `sync.Pool` позволяет зафиксировать количество потребляемой памяти.

При желании пул можно сделать "резиновым".

	p := pool.New(128) // 128 goroutines pool.
	
	epoll.Add(ch.conn, EPOLLIN, func() {
		pool.Schedule(ch.Receive)
	})

* Write 

С `Write()` дело обстоит проще, так как мы всегда знаем, когда хотим что-то записать в соединение.

Сложный момент – синхронизация отправляемых пакетов:

.code media/code/02_write.go /START/,/END/

* github.com/gobwas/ws

* Пришло время запилить свою либу

Как уже говорилось выше, в Go существуют две реализации WebSocket'ов.

И обе они [[https://github.com/gorilla/websocket/issues/186][не позволяют]] полностью контролировать работу с `[]byte` и прочими буферами.

Поэтому пришлось реализовать RFC6455 с более низкоуровневым API:

	import "github.com/gobwas/ws"

	http.HandleFunc("/", func(res http.ResponseWriter, req *http.Request) {
		conn, _ := ws.Upgrade(req, res)
		
		ws.ReadFrame(conn) 
		ws.WriteFrame(conn, ws.NewTextFrame(...))
	})
	
* Zero-copy Upgrade

Зачем нужна куча аллокаций внутри `net/http`, когда нам нужно все это только один раз?

	ln, _ := net.Listen("tcp", ":8888")

	for {
		conn, _ := ln.Accept()
		ws.UpgradeConn(conn)
		// ...
	}

// Benchmarks here


* Zero copy upgrade

При использовании с `pool` можно избегать плохих overload ситуаций:

	ln, _ := net.Listen("tcp", ":8888")

	for {
		conn, _ := ln.Accept()

		err := pool.ScheduleTimeout(func() {
			ws.UpgradeConn(conn)
			// ...
		})
		if err != nil {
			writeNextUpstreamError(conn)
			
			// Handle temporary error like net/http does.
			time.Sleep(...)
		}
	}

* Nginx

- Разделить соединения одного процесса на несколько;
- Помогает с DDOS;
- Использовать `proxy_next_upstream`

* Graceful restart

- supervisord
- start N instances, wait, stop N instances;
- shutdown message in all parties;
